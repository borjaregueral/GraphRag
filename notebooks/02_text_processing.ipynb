{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import nltk\n",
    "import spacy\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Load spaCy for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 6  # Adjust based on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NER pipeline\n",
    "model_name = \"xlm-roberta-large-finetuned-conll03-english\"  # Your chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    aggregation_strategy=\"max\", \n",
    "    device=0  # Set to -1 if using CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiktoken_len(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")  # Or your model name\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def merge_split_sentences(chunks):\n",
    "    merged_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for chunk in tqdm(chunks, desc=\"Merging Sentences\"):\n",
    "        sentences = nltk.sent_tokenize(chunk['chunk_text'])  # Use 'chunk_text' instead of 'text'\n",
    "        if not current_chunk: \n",
    "            current_chunk = sentences[0] \n",
    "            sentences = sentences[1:]\n",
    "        for sentence in sentences:\n",
    "            if len(nltk.word_tokenize(current_chunk + \" \" + sentence)) <= 4000:  # Example word limit\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                merged_chunks.append({**chunk, 'chunk_text': current_chunk})  # Keep other chunk data\n",
    "                current_chunk = sentence\n",
    "    if current_chunk: \n",
    "        merged_chunks.append({**chunk, 'chunk_text': current_chunk})\n",
    "    return merged_chunks\n",
    "\n",
    "def extract_entities_transformer(texts, ner_pipeline):\n",
    "    return ner_pipeline(texts)\n",
    "\n",
    "def refine_entities(entities):\n",
    "    refined_entities = [\n",
    "        [\n",
    "            (\" \".join([token.lemma_ for token in nlp(word)]), label)\n",
    "            for word, label in entity_list\n",
    "            if label != 'O' and len(word.split()) <= 5\n",
    "        ]\n",
    "        for entity_list in entities\n",
    "    ]\n",
    "    return refined_entities\n",
    "\n",
    "# Function to process a single batch\n",
    "def process_batch(batch_texts):\n",
    "    try:\n",
    "        batch_entities = extract_entities_transformer(batch_texts, ner_pipeline)\n",
    "        refined = [\n",
    "            [(entity['word'], entity['entity_group']) for entity in text_entities]\n",
    "            for text_entities in batch_entities\n",
    "        ]\n",
    "        return refine_entities(refined)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [[] for _ in batch_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts_clean = pl.read_parquet(\"/Users/borja/Documents/Somniumrema/projects/genai/grag/pipeline_outcomes/podcasts_clean.parquet\")\n",
    "\n",
    "podcasts_clean = podcasts_clean[['post_url', 'post_title', 'series_number', 'blog_date',\n",
    "                                 'blog_title', 'file_name', 'cleaned_text','tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,  # Adjust based on your language model's context window\n",
    "    chunk_overlap=1000,\n",
    "    length_function=tiktoken_len, \n",
    "    separators=[\". \", \"!\", \"?\", \"\\n\\n\", \"\\n\", \" \", \"\"] \n",
    ")\n",
    "\n",
    "# Create chunks for each transcript while preserving DataFrame structure using list comprehension\n",
    "all_chunks = [\n",
    "    {**row, 'chunk_id': i, 'chunk_text': chunk.page_content,  'tokens': tiktoken_len(chunk.page_content)}\n",
    "    for row in podcasts_clean.iter_rows(named=True)\n",
    "    for i, chunk in enumerate(text_splitter.create_documents([row['cleaned_text']]))\n",
    "]\n",
    "\n",
    "# Merge split sentences\n",
    "all_chunks = merge_split_sentences(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the chunks and original columns, excluding 'cleaned_text'\n",
    "chunks_df = pl.DataFrame(all_chunks).drop(['cleaned_text', 'chunk_id']) \n",
    "\n",
    "# Verify the new DataFrame\n",
    "chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'chunk_text' as a list for batch processing\n",
    "texts = chunks_df['chunk_text'].to_list()\n",
    "\n",
    "# Create list of batches\n",
    "batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "\n",
    "# Initialize list to store all entities\n",
    "entities = []\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for refined_entities in tqdm(executor.map(process_batch, batches), total=len(batches), desc=\"Extracting Entities\"):\n",
    "        entities.extend(refined_entities)\n",
    "\n",
    "# Add the 'entities' column to the DataFrame\n",
    "chunks_df = chunks_df.with_columns([\n",
    "    pl.Series(\"entities\", entities)\n",
    "])\n",
    "\n",
    "# Verify the new DataFrame with entities\n",
    "chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df.write_parquet(\"/Users/borja/Documents/Somniumrema/projects/genai/grag/pipeline_outcomes/chunks_df.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grag-EOKyDehK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
